{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SLiBxjBJ9IMs"
   },
   "source": [
    "# Часть 1. Обучение с учителем\n",
    "\n",
    "**Обучение с учителем** — это направление машинного обучения, объединяющее алгоритмы и методы построения моделей на основе множества примеров, содержащих пары \"известный вход - известный выход\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iABsxWwB9IM6"
   },
   "source": [
    "Центральным понятием машинного обучения является _обучающая выборка_.  \n",
    "**Обучающая выборка** — набор структурированных данных, используемый для обучения моделей. Это примеры, на основе которых мы планируем строить общую закономерность. Она обозначается $X$ и состоит из $l$ пар объектов $x_{i}$ и известных ответов $y_{i}$:\n",
    "\n",
    "$$X = (x^{i}, y_{i})^l_{i=1}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F0Fh0yJn9IM7"
   },
   "source": [
    "Функция, отображающая пространство объектов $\\mathbb{X}$ в пространство ответов $\\mathbb{Y}$, помогающая нам делать предсказания, называется _алгоритмом_ или _моделью_ и обозначается $a(x)$. Она принимает на вход объект и выдает ответ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BVlLfs339IM8"
   },
   "source": [
    "Для решения определенной задачи не все алгоритмы одинаково хорошо подходят. Для определения наиболее подходящего алгоритма введена характеристика, называемая _функционалом ошибки_ $Q(a, X)$. Он принимает на вход алгоритм и выборку и сообщает, насколько хорошо данный алгоритм работает на данной выборке. В примере распознавания спам-писем в качестве такого функционала может выступать доля неправильных ответов (предсказаний). Задача обучения заключается в подборе такого алгоритма, при котором достигается минимум функционала ошибки $Q(a, X)\\rightarrow min.$\n",
    "\n",
    "Наиболее подходящий алгоритм при этом выбирается из множества, называемого _семейством алгоритмов_ $\\mathbb{A}$. Их мы будем рассматривать в данном курсе."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QCfFle3N9INA"
   },
   "source": [
    "## Линейная регрессия. MAE, MSE<a class=\"anchor\" id=\"1\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bLvKfPfN9INB"
   },
   "source": [
    "Линейные модели - это такие модели, которые сводятся к суммированию значений признаков с некоторыми весами. Само название модели говорит о том, что зависимость предсказываемой переменной от признаков будет линейной:\n",
    "\n",
    "$$a(x) = w_{0}+\\sum^{d}_{i=1}w_{i}x^{i}.$$\n",
    "\n",
    "**Линейная регрессия** — модель зависимости переменной x от одной или нескольких других переменных (факторов, регрессоров, независимых переменных) с линейной функцией зависимости.\n",
    "\n",
    "В данном случае параметрами моделей являются веса $w_{i}$. Вес $w_{0}$ называется _свободным коэффициентом_ или _сдвигом_. Оптимизация модели в таком случае заключается в подборе оптимальных значений весов. Сумму в формуле также можно описать как скалярное произведение вектора признаков $x=(x^{1},...,x^{d})$ на вектор весов $w=(w_{1},...,w_{d})$:\n",
    "\n",
    "$$a(x) = w_{0}+\\left \\langle w,x \\right \\rangle.$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обратим внимание, что сдвиг делает модель неоднородной и затрудняет ее дальнейшую оптимизацию. Для устранения этого фактора обычно используют прием, позволяющий упростить запись: к признаковому описанию объекта добавляется еще один признак (константный), на каждом объекте равный единице. В этом случае вес при нем как раз будет по смыслу совпадать со свободным коэффициентом, и сам $w_{0}$ будет не нужен. Тогда получим\n",
    "\n",
    "$$a(x) = \\sum^{d+1}_{i=1}w_{i}x^{i}=\\left \\langle w,x \\right \\rangle.$$\n",
    "\n",
    "За счет простой формы линейные модели достаточно легко обучаются и позволяют работать с зашумленными данными, небольшими выборками, контролирауя при этом риск переобучения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i8v9_4TB9INC"
   },
   "source": [
    "Для обучения модели необходимо иметь возможность измерять точность линейного алгоритма на выборке (обучающей или тестовой). \n",
    "\n",
    "В качестве меры ошибки можно взять абсолютное отклонение истинного значения от прогноза $Q(a,y)=a(x)-y$, но тогда минимизация функционала ошибки (в которой и состоит задача обучения) будет достигаться при принятии им отрицательного значения. Например, если истинное значение ответа равно $10$, а алгоритм $a(x)$ выдает ответ $11$, отклонение будет равно $1$, а при значении предсказания равном $1$, отклонение будет равно $1-10=-9$. Значение ошибки во втором случае ниже, однако разница истинного значения и предсказания нашей модели больше. Таким образом, такой функционал ошибки не поддается минимизации. \n",
    "\n",
    "Логичным кажется решение использовать в качестве функционала ошибки модуль отклонения $Q(a,y)=|a(x)-y|$. Соответствующий функционал ошибки называется средним абсолютным отклонением (mean absolute error, MAE):\n",
    "\n",
    "$$Q(a,x) = \\frac{1}{l}\\sum^{l}_{i=1}|a(x_{i})-y_{i}|.$$\n",
    "\n",
    "Однако, как мы далее увидим, зачастую методы оптимизации включают в себя дифференцирование, а функция модуля не является гладкой - она не имеет производной в нуле, поэтому ее оптимизация бывает затруднительной.\n",
    "\n",
    "Поэтому сейчас основной способ измерить отклонение - посчитать квадрат разности $Q(a,y)=(a(x)-y)^{2}$. Такая функция является гладкой и имеет производную в каждой точке, а ее минимум достигается при равенстве истинного ответа $y$ и прогноза $a(x)$.\n",
    "\n",
    "Основанный на этой функции функционал ошибки называется _среднеквадратичным отклонением_ (mean squared error, MSE):\n",
    "\n",
    "$$Q(a,x) = \\frac{1}{l}\\sum^{l}_{i=1}(a(x_{i})-y_{i})^{2}.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_mae(y, y_pred):\n",
    "    err = np.mean(np.abs(y - y_pred))\n",
    "    return err\n",
    "\n",
    "def calc_mse(y, y_pred):\n",
    "    err = np.mean((y - y_pred)**2)\n",
    "    return err"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qRwG-e0m9IND"
   },
   "source": [
    "## Метод наименьших квадратов<a class=\"anchor\" id=\"2\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JpzXCWHG9INE"
   },
   "source": [
    "Как уже говорилось ранее, обучение модели регрессии заключается в минимизации функционала ошибки. Таким образом, в случае использования среднеквадратичной ошибки получаем задачу оптимизации\n",
    "\n",
    "$$Q(w,x) = \\frac{1}{l}\\sum^{l}_{i=1}(\\left \\langle w,x_{i} \\right \\rangle-y_{i})^{2} \\rightarrow \\underset{w}{\\text{min}}.$$\n",
    "\n",
    "Способ вычисления весов путем минимизации среднеквадратичного отклонения называется **методом наименьших квадратов**.\n",
    "\n",
    "Заметим, что здесь мы переписали выражение функционала ошибки, заменив $a(x)$ на скалярное призведение $\\left \\langle w,x \\right \\rangle$, после чего мы уже имеем функцию, а не функционал ошибки, так как $Q$ зависит не от некоторой функции $a(x)$, а от вектора весов $w$, и оптимизировать нужно именно по нему, что гораздо проще."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eC50nWZ69INE"
   },
   "source": [
    "Имеет смысл переписать имеющиеся соотношения в матричном виде. В матрицу \"объекты-признаки\" впишем по строкам $d$ признаков для всех $l$ объектов из обучающей выборки: \n",
    "\n",
    "$$X = \\begin{pmatrix}\n",
    "x_{11} & ... & x_{1d}\\\\ \n",
    "... & ... & ...\\\\ \n",
    "x_{l1} & ... & x_{ld}\n",
    "\\end{pmatrix},$$\n",
    "\n",
    "и составим вектор ответов $y$ из истинных ответов для данной выборки:\n",
    "\n",
    "$$y = \\begin{pmatrix}\n",
    "y_{1}\\\\ \n",
    "...\\\\ \n",
    "y_{l}\n",
    "\\end{pmatrix}.$$\n",
    "\n",
    "Помня, что $w$ - вектор параметров, переписанная в матричном виде задача будет выглядеть следующим образом:\n",
    "\n",
    "$$Q(w, X) = \\frac{1}{l}||Xw-y||^{2}\\rightarrow \\underset{w}{\\text{min}},$$\n",
    "\n",
    "где используется евклидова ($L_{2}$) норма:\n",
    "\n",
    "$$||x|| = \\sqrt{\\sum_{i=1}^l{x_i^2}} $$\n",
    "\n",
    "$$||Xw-y|| = \\sqrt{\\sum_{i=1}^l{(X_iw-y_i)^2}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$Q(w, X) = \\frac{1}{l}\\sqrt{\\sum_{i=1}^l{(X_iw-y_i)^2}} ^{2} = \\frac{1}{l}\\sum_{i=1}^l{(X_iw-y_i)^2}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\nabla_{w}Q(w,X) = \\frac{2}{l}X^{T}(Xw-y).$\n",
    "\n",
    "$\\frac{2}{l}X^{T}(Xw-y) = 0$\n",
    "\n",
    "$X^{T}(Xw-y) = 0$\n",
    "\n",
    "$X^{T}Xw-X^{T}y = 0$\n",
    "\n",
    "$w = \\frac{X^{T}y}{X^{T}X}$\n",
    "\n",
    "$w = (X^{T}X)^{-1}X^{T}y $\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1eW4aV609ING"
   },
   "source": [
    "Продифференцировав данную функцию по вектору $w$ и приравняв к нулю, можно получить явную анатилическую формулу для решения задачи минимизации ([ссылка  (см. пункт 1.2)](https://habr.com/ru/company/ods/blog/323890/#metod-naimenshih-kvadratov) на подробный вывод аналитической формулы решения уравнения линейной регрессии):\n",
    "\n",
    "$$w = (X^{T}X)^{-1}X^{T}y.$$\n",
    "\n",
    "Это решение называется _нормальным уравнением_ линейной регрессии. Наличие аналитического решения кажется положительным фактором, однако, у него есть некоторые минусы, среди которых вычислительная сложность операции (обращение матрицы $X^{T}X$ будет иметь кубическую сложность от количества признаков $d^{3}$), а также тот факт, что матрица $X^{T}X$ может быть вырожденной и поэтому необратимой. Тогда найти решение будет невозможно.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d.axes3d import Axes3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1234)\n",
    "# Возьмем 2 признака и 1000 объектов\n",
    "n_features = 2\n",
    "n_objects = 1000\n",
    "\n",
    "# сгенерируем вектор истинных весов\n",
    "w_true = np.random.normal(size=(n_features))\n",
    "\n",
    "# сгенерируем матрицу X, вычислим Y с добавлением случайного шума\n",
    "X = np.random.uniform(-7, 7, (n_objects, n_features))\n",
    "Y = X.dot(w_true) + np.random.normal(0, 0.5, size=(n_objects))\n",
    "\n",
    "# возьмем нулевые начальные веса\n",
    "w = np.zeros(n_features)\n",
    "\n",
    "display(w, w_true, X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,10))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "ax.scatter(X[:, 0], X[:, 1], Y)\n",
    "\n",
    "ax.set_xlabel('X0')\n",
    "ax.set_ylabel('X1')\n",
    "ax.set_zlabel('Y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# реализуем функцию, определяющую среднеквадратичную ошибку\n",
    "def mserror(X, w, y):\n",
    "    y_pred = X.dot(w)\n",
    "    return (np.sum((y_pred - y)**2)) / len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Реализуем функцию, вычисляющую вектор весов по нормальному уравнению линейной регрессии\n",
    "normal_eq_w = np.linalg.inv(np.dot(X.T, X)) @ X.T @ Y\n",
    "print(f'Веса {normal_eq_w}')\n",
    "print(f'В случае использования нормального уравнения функционал ошибки составляет ', end='')\n",
    "print(f'{round(mserror(X, normal_eq_w, Y), 4)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Линейная регрессия - простой, но зачастую эффективный, способ приближать вещественную целевую переменную через линейную комбинацию признаков\n",
    "* Решение регрессии - МНК, можно решать аналитически, но на практике - градиентный спуск (GD, Gradient Descent)  \n",
    "\n",
    "[Вывод аналитической формулы решения уравнения линейной регрессии](https://habr.com/ru/company/ods/blog/323890/#metod-naimenshih-kvadratov) (см. пункт 1.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nfRgH2cC9INI"
   },
   "source": [
    "## Градиентный спуск<a class=\"anchor\" id=\"3\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fgHZFo-x9INK"
   },
   "source": [
    "Более удобным подходом будет разработка решения с помощью численных методов оптимизации, одним из которых является _градиентный спуск_.\n",
    "\n",
    "Среднеквадратичная ошибка имеет один минимум и непрерывна на всей области значений (то есть является выпуклой и гладкой), а значит в каждой ее точке можно посчитать частные производные.\n",
    "\n",
    "Вспомним, что _градиентом_ функции $f$ называется $n$-мерный вектор из частных производных. \n",
    "\n",
    "$$ \\nabla f(x_{1},...,x_{d}) = \\left(\\frac{\\partial f}{\\partial x_{i}}\\right)^{d}_{i=1}.$$\n",
    "\n",
    "При этом известно, что __градиент задает направление наискорейшего роста функции__. Значит, антиградиент будет показывать направление ее скорейшего убывания, что будет полезно нам в нашей задаче минимизации функционала ошибки. \n",
    "\n",
    "**Градиентный спуск** — метод нахождения локального экстремума функции (минимума или максимума) с помощью движения вдоль градиента."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EITeLDLk9INM"
   },
   "source": [
    "Для решения задачи нам требуется определить некоторую стартовую точку и итерационно сдвигаться от нее в сторону антиградиента с определенным _шагом_ $\\eta_{k}$, на каждом шагу пересчитывая градиент в точке, в которой мы находимся. Таким образом, имея начальный вектор весов $w^{0}$, $k$-й шаг градиентного спуска будет иметь вид\n",
    "\n",
    "$$w^{k} = w^{k-1} - \\eta_{k}\\nabla Q(w^{k-1}, X).$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AD9YpghL9INO"
   },
   "source": [
    "Итерации следует продолжать, пока не наступает сходимость. Она определяется разными способами, но в даннном случае удобно определять как ситуацию, когда векторы весов от шага к шагу изменяются незначительно, то есть норма отклонения вектора весов на текущем шаге от предыдущего не привышает заданное значение $\\varepsilon$:\n",
    "\n",
    "$$||w^{k}-w^{k-1}|| < \\varepsilon.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b4yKBld_9INT"
   },
   "source": [
    "Начальный вектор весов $w_{0}$ также можно определять различными способами, обычно его берут нулевым или состоящим из случайных небольших чисел."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GhuJyO__9INU"
   },
   "source": [
    "В случае многомерной регрессии (при количестве признаков больше 1) при оптимизации функционала ошибки \n",
    "\n",
    "$$Q(w, X) = \\frac{1}{l}||Xw-y||^{2}\\rightarrow \\underset{w}{\\text{min}}$$\n",
    "\n",
    "формула вычисления градиента принимает вид\n",
    "\n",
    "$$\\nabla_{w}Q(w,X) = \\frac{2}{l}X^{T}(Xw-y).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Инициализация w\n",
    "\n",
    "2. Цикл по k = 1,2,3,...:\n",
    "\n",
    "    * $w^{k} = w^{k-1} - \\eta_{k}\\nabla Q(w^{k-1}, X)$\n",
    "\n",
    "    * Если $||w^{k} - w^{k-1}|| < \\epsilon$, то завершить.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Cqw7Fffj9IN0"
   },
   "source": [
    "Обучим линейную регрессию путем градиентного спуска и получим графики изменения весов и ошибки\n",
    "\n",
    "1. Инициализация w\n",
    "\n",
    "2. Цикл по k = 1,2,3,...:\n",
    "\n",
    "    * $w^{k} = w^{k-1} - \\eta_{k}\\nabla Q(w^{k-1}, X)$\n",
    "\n",
    "    * Если $||w^{k} - w^{k-1}|| < \\epsilon$, то завершить.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2vnzIkxt9IN1",
    "outputId": "28f71598-e128-4adb-da62-1601401800f1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0: error - 12.198224910563486, weights: [ 0.13477638 -0.38857461]\n",
      "Iter 1: error - 5.721927562708119, weights: [ 0.23166602 -0.64911175]\n",
      "Iter 2: error - 2.7571390971724696, weights: [ 0.30088498 -0.82395109]\n",
      "Iter 3: error - 1.397693145680423, weights: [ 0.350075   -0.94137792]\n",
      "Iter 4: error - 0.7734530790003223, weights: [ 0.38487408 -1.02030733]\n",
      "Iter 5: error - 0.4864474154164299, weights: [ 0.40939674 -1.07340045]\n",
      "Iter 6: error - 0.354344231544237, weights: [ 0.42661932 -1.10914011]\n",
      "Iter 7: error - 0.2934800582148447, weights: [ 0.4386792  -1.13321479]\n",
      "Iter 8: error - 0.265413850692826, weights: [ 0.447102   -1.14944239]\n",
      "Iter 9: error - 0.2524619639743879, weights: [ 0.45297107 -1.16038742]\n",
      "Iter 10: error - 0.24648103454886586, weights: [ 0.45705232 -1.16777387]\n",
      "Iter 11: error - 0.24371756626897814, weights: [ 0.45988515 -1.17276152]\n",
      "Iter 12: error - 0.24244007305388324, weights: [ 0.46184823 -1.17613117]\n",
      "Iter 13: error - 0.2418492559727374, weights: [ 0.4632066  -1.17840885]\n",
      "Iter 14: error - 0.24157590963593276, weights: [ 0.46414529 -1.17994914]\n",
      "Iter 15: error - 0.241449401703911, weights: [ 0.46479317 -1.18099124]\n",
      "Iter 16: error - 0.241390835409215, weights: [ 0.46523987 -1.18169657]\n",
      "Iter 17: error - 0.241363715587635, weights: [ 0.46554754 -1.18217416]\n",
      "Iter 18: error - 0.241351154689716, weights: [ 0.46575927 -1.18249766]\n",
      "Iter 19: error - 0.2413453358438324, weights: [ 0.46590486 -1.18271686]\n",
      "Iter 20: error - 0.24134263981533452, weights: [ 0.46600489 -1.18286544]\n",
      "Iter 21: error - 0.24134139049412393, weights: [ 0.46607357 -1.18296618]\n",
      "Iter 22: error - 0.24134081149547823, weights: [ 0.4661207  -1.18303451]\n",
      "Iter 23: error - 0.2413405431293834, weights: [ 0.46615302 -1.18308086]\n",
      "Iter 24: error - 0.24134041873000472, weights: [ 0.46617518 -1.18311232]\n",
      "Iter 25: error - 0.2413403610608135, weights: [ 0.46619035 -1.18313367]\n",
      "Iter 26: error - 0.24134033432459853, weights: [ 0.46620075 -1.18314816]\n",
      "Iter 27: error - 0.2413403219285762, weights: [ 0.46620786 -1.183158  ]\n",
      "Iter 28: error - 0.24134031618096188, weights: [ 0.46621273 -1.18316469]\n",
      "Iter 29: error - 0.24134031351586688, weights: [ 0.46621606 -1.18316923]\n",
      "Iter 30: error - 0.2413403122800478, weights: [ 0.46621834 -1.18317231]\n",
      "Iter 31: error - 0.24134031170697204, weights: [ 0.46621989 -1.18317441]\n",
      "Iter 32: error - 0.24134031144121676, weights: [ 0.46622096 -1.18317584]\n",
      "Iter 33: error - 0.2413403113179735, weights: [ 0.46622169 -1.1831768 ]\n",
      "Iter 34: error - 0.2413403112608185, weights: [ 0.46622218 -1.18317746]\n",
      "Iter 35: error - 0.24134031123431202, weights: [ 0.46622252 -1.18317791]\n",
      "Iter 36: error - 0.24134031122201902, weights: [ 0.46622275 -1.18317822]\n",
      "Iter 37: error - 0.24134031121631766, weights: [ 0.46622291 -1.18317842]\n",
      "Iter 38: error - 0.24134031121367355, weights: [ 0.46622302 -1.18317856]\n",
      "Iter 39: error - 0.24134031121244717, weights: [ 0.46622309 -1.18317866]\n",
      "Iter 40: error - 0.24134031121187843, weights: [ 0.46622314 -1.18317872]\n",
      "Iter 41: error - 0.24134031121161462, weights: [ 0.46622318 -1.18317877]\n",
      "Iter 42: error - 0.24134031121149224, weights: [ 0.4662232 -1.1831788]\n",
      "Iter 43: error - 0.2413403112114355, weights: [ 0.46622322 -1.18317882]\n",
      "Iter 44: error - 0.2413403112114092, weights: [ 0.46622323 -1.18317883]\n",
      "Iter 45: error - 0.24134031121139696, weights: [ 0.46622323 -1.18317884]\n",
      "Iter 46: error - 0.24134031121139135, weights: [ 0.46622324 -1.18317885]\n",
      "В случае использования градиентного спуска функционал ошибки составляет 0.2413\n"
     ]
    }
   ],
   "source": [
    "# список векторов весов после каждой итерации\n",
    "w_list = [w.copy()]\n",
    "\n",
    "# список значений ошибок после каждой итерации\n",
    "errors = []\n",
    "\n",
    "# шаг градиентного спуска\n",
    "eta = 0.01\n",
    "\n",
    "# максимальное число итераций\n",
    "max_iter = 1e4\n",
    "\n",
    "# критерий сходимости (разница весов, при которой алгоритм останавливается)\n",
    "min_weight_dist = 1e-8\n",
    "\n",
    "# зададим начальную разницу весов большим числом\n",
    "weight_dist = np.inf\n",
    "\n",
    "# счетчик итераций\n",
    "iter_num = 0\n",
    "\n",
    "# ход градиентного спуска\n",
    "while weight_dist > min_weight_dist and iter_num < max_iter:\n",
    "    y_pred = np.dot(X, w)\n",
    "    dQ = 2 / Y.shape[0] * np.dot(X.T, y_pred - Y)\n",
    "    new_w = w - eta * dQ\n",
    "    weight_dist = np.linalg.norm(new_w - w, ord=2)\n",
    "    error = mserror(X, new_w, Y)\n",
    "    \n",
    "    w_list.append(new_w.copy())\n",
    "    errors.append(error)\n",
    "    \n",
    "    print(f'Iter {iter_num}: error - {error}, weights: {new_w}')\n",
    "    \n",
    "    iter_num += 1\n",
    "    w = new_w\n",
    "    \n",
    "w_list = np.array(w_list)\n",
    "w_pred = w_list[-1]\n",
    "\n",
    "print(f'В случае использования градиентного спуска функционал ошибки составляет {round(errors[-1], 4)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Afahfb849IN4",
    "outputId": "f1eb65a2-f819-49d3-8cb6-491d06b01d49"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 936x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Визуализируем изменение весов (красной точкой обозначены истинные веса, сгенерированные вначале)\n",
    "plt.figure(figsize=(13, 6))\n",
    "plt.title('Gradient descent')\n",
    "plt.xlabel(r'$w_1$')\n",
    "plt.ylabel(r'$w_2$')\n",
    "\n",
    "plt.scatter(w_list[:, 0], w_list[:, 1])\n",
    "plt.scatter(w_true[0], w_true[1], c='r')\n",
    "plt.plot(w_list[:, 0], w_list[:, 1])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MiTINBlK9IN7"
   },
   "source": [
    "После каждой итерации значения искомых весов приближаются к истинным, однако, не становятся им равны из-за шума, добавленного в вектор ответов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lFzwe8pW9IN8",
    "outputId": "511c62c8-8ee3-40cc-ac09-be80875cc9ab"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'MSE')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Визуализируем изменение функционала ошибки\n",
    "plt.plot(range(len(errors)), errors)\n",
    "plt.title('MSE')\n",
    "plt.xlabel('Iteration number')\n",
    "plt.ylabel('MSE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0Kcz4wFx9IOB"
   },
   "source": [
    "Видно, что изменение монотонно и начинается с высокой точки, после определенного количества итераций выходя на асимптоту."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "noIKD8ls9IOD"
   },
   "source": [
    "Очень важно при использовании метода градиентного спуска правильно подбирать шаг. Если длина шага будет слишком мала, то метод будет слишком медленно приближаться к правильному ответу, и потребуется очень большое количество итераций для достижения сходимости. Если же длина наоборот будет слишком большой, появится вероятность \"перепрыгивания\" алгоритма через минимум функции или вообще отсутствия сходимости градиентного спуска.\n",
    "\n",
    "Применяется методика использования переменного размера шага: на начальных этапах берется большой шаг, который с увеличением количества итераций снижается. Одна из таких методик - вычисление на каждой итерации размера шага по формуле\n",
    "\n",
    "$$\\eta_{k} = \\frac{c}{k},$$\n",
    "\n",
    "где $c$ - некоторая константа, а $k$ - номер шага."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* MSE удобна для градиентного спуска, так как дифференцируема\n",
    "* Максимальное качество градиентного спуска достигается малыми шагами и большим кол-вом итераций  \n",
    "\n",
    "[Математическое описание метода градиентного спуска](http://www.machinelearning.ru/wiki/index.php?title=%D0%9C%D0%B5%D1%82%D0%BE%D0%B4_%D0%B3%D1%80%D0%B0%D0%B4%D0%B8%D0%B5%D0%BD%D1%82%D0%BD%D0%BE%D0%B3%D0%BE_%D1%81%D0%BF%D1%83%D1%81%D0%BA%D0%B0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Опеределения\n",
    "\n",
    "**Обучение без учителя** — направление машинного обучения, в которой для коррекции параметров обучаемой модели не используется целевая функция. Иными словами, в обучающих примерах при обучении без учителя не нужно иметь заранее заданные выходы модели.\n",
    "\n",
    "**Обучение с подкреплением** — раздел машинного обучения, изучающий поведение интеллектуальных агентов, действующих в некоторой среде и принимающих решения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Масштабирование признаков "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Многие методы машинного обучения, в том числе и линейные, наиболее эффективны в том случае, когда признаки имеют одинаковый масштаб. По сути масштабирование означает приведение признаков к какой-то единой шкале. \n",
    "Важным и последним свойством масштабирования является факт, что после масштабирования признаков в линейных моделях веса при них могут интерпретироваться как мера значимости этих признаков.\n",
    "\n",
    "Существуют различные ситуации, когда целесообразно применять тот или иной метод масштабирования. Нормализовать полезно признаки, опирающиеся на величину значений - такие как расстояние (knn, k-means). Стандартизировать полезно признаки для модели, которая опирается на распределение (линейные модели). В общем случае, когда выбор метода неочевиден, полезной практикой считается создавать масштабированные копии набора данных, с которыми работает специалист, и сравнивать друг с другом полученные после применения модели результаты для выявления оптимального метода масштабирования для имеющейся ситуации.\n",
    "\n",
    "Метод **нормализации** заключается в приведении признаков к масштабу в диапазоне [0-1].\n",
    "\n",
    "Для его реализации необходимо найти минимальное $min_{j} (x^{j}_{i})$ и максимальное $max_{j} (x^{j}_{i})$ значение признака на обучающей выборке. При этом отмасштабированное значение признака будет находиться по формуле\n",
    "\n",
    "$$x^{j}_{i} = \\frac{x^{j}_{i} - min_{j} (x^{j}_{i})}{max_{j} (x^{j}_{i})-min_{j} (x^{j}_{i})}.$$\n",
    "\n",
    "После преобразования значений признаков минимальное значение превратится в 0, а максимальное - в 1.\n",
    "\n",
    "**Стандартизация** заключается в получении своего рода значения сдвига каждого признака от среднего. Для ее реализации необходимо вычислить среднее значение признака \n",
    "\n",
    "$$\\mu_{j} = \\frac{1}{l}\\sum^{l}_{i=1}x^{j}_{i}$$\n",
    "\n",
    "и стандартное отклонение, которое находится путем суммирования квадратов отклонения значений признака на объектах выборки от среднего $\\mu_{j}$ и делением на число объектов выборки с последующим извлечением корня:\n",
    "\n",
    "$$\\sigma_{j} = \\sqrt{\\frac{1}{l}\\sum^{l}_{i=1}(x^{j}_{i}-\\mu_{j})^{2}}$$\n",
    "\n",
    "Чтобы отмасштабировать признак, каждое его значение преобразуется по формуле\n",
    "\n",
    "$$x^{j}_{i}=\\frac{x^{j}_{i} - \\mu_{j}}{\\sigma_{j}}.$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_max_scale(X):\n",
    "    return (X - X.min()) / (X.max() - X.min())\n",
    "\n",
    "def standard_scale(X):\n",
    "    mean = X.mean()\n",
    "    std = X.std()\n",
    "    return (X - mean) / std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Стохастический градиентный спуск <a class='anchor' id='1'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Один из главных недостатков градиентного спуска: если выборка большая по объему, то даже один шаг градиентного спуска будет занимать много вычислительных ресурсов и времени.\n",
    "\n",
    "Стремление к оптимизации процесса привело к появлению _стохастического градиентного спуска_ (Stochastic gradient descent, SGD). Идея его основана на том, что на одной итерации мы вычитаем не вектор градиента, вычисленный по всей выборке, а вместо этого случайно выбираем один объект из обучающей выборки $x_{i}$ и вычисляем градиент только на этом объекте, то есть градиент только одного слагаемого в функционале ошибки и вычитаем именно этот градиент из текущего приближения вектора весов:\n",
    "\n",
    "$$w^{k} = w^{k-1} - \\eta_{k}\\nabla Q(w^{k-1}, \\{x_{i}\\}),$$\n",
    "\n",
    "то есть $\\nabla Q(w^{k-1}, X)$ заменяется на $\\nabla Q(w^{k-1}, \\{x_{i}\\})$.  \n",
    "\n",
    "Если в случае градиентного спуска мы стараемся на каждой итерации уменьшить ошибку на всей выборке, и по мере увеличения числа итераций ошибка падает монотонно, то в случае стохастического градиентного спуска мы уменьшаем на каждой итерации ошибку только на одном объекте, но при этом есть вероятность увеличить ее на другом объекте, поэтому график изменения ошибки может получаться немонотонным, и даже иметь пики (см. пример по ссылке [1] из списка литературы). То есть на какой-то итерации мы можем даже увеличить ошибку, но при этом в целом по ходу метода ошибка снижается, и рано или поздно мы выходим на нормальный уровень.  \n",
    "\n",
    "1. Инициализация w\n",
    "\n",
    "2. Цикл по k = 1,2,3,...:\n",
    "\n",
    "    * Выбираем случайные объект $x_{i}$ из X\n",
    "    * $w^{k} = w^{k-1} - \\eta_{k}\\nabla Q(w^{k-1}, \\{x_{i}\\})$\n",
    "    * Если $||w^{k} - w^{k-1}|| < \\epsilon$, то завершить."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# сгенерируем набор данных\n",
    "X, Y, coef = datasets.make_regression(n_samples=1000, n_features=2, n_informative=2, n_targets=1, \n",
    "                                      noise=5, coef=True, random_state=2)\n",
    "X[:, 0] *= 10\n",
    "display(X, Y, coef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# реализуем функцию, определяющую среднеквадратичную ошибку\n",
    "def mserror(X, w, y_pred):\n",
    "    y = X.dot(w)\n",
    "    return (sum((y - y_pred)**2)) / len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# инициализируем начальный вектор весов\n",
    "w = np.zeros(X.shape[1])\n",
    "\n",
    "# список векторов весов после каждой итерации\n",
    "w_list = [w.copy()]\n",
    "\n",
    "# список значений ошибок после каждой итерации\n",
    "errors = []\n",
    "\n",
    "# шаг градиентного спуска\n",
    "eta = 0.01\n",
    "\n",
    "# максимальное число итераций\n",
    "max_iter = 1e3\n",
    "\n",
    "# критерий сходимости (разница весов, при которой алгоритм останавливается)\n",
    "min_weight_dist = 1e-8\n",
    "\n",
    "# зададим начальную разницу весов большим числом\n",
    "weight_dist = np.inf\n",
    "\n",
    "# счетчик итераций\n",
    "iter_num = 0\n",
    "\n",
    "np.random.seed(1234)\n",
    "\n",
    "# ход градиентного спуска\n",
    "while weight_dist > min_weight_dist and iter_num < max_iter:\n",
    "    \n",
    "    # генерируем случайный индекс объекта выборки\n",
    "    train_ind = np.random.randint(X.shape[0], size=1)\n",
    "    \n",
    "    y_pred = np.dot(X[train_ind], w)\n",
    "    new_w = w - eta * 2 / Y[train_ind].shape[0] * np.dot(X[train_ind].T, y_pred - Y[train_ind])\n",
    "\n",
    "    weight_dist = np.linalg.norm(new_w - w, ord=2)\n",
    " \n",
    "    error = mserror(X, new_w, Y)\n",
    "    \n",
    "    w_list.append(new_w.copy())\n",
    "    errors.append(error)\n",
    "    \n",
    "    if iter_num % 100 == 0:\n",
    "        print(f'Iteration #{iter_num}: W_new = {new_w}, MSE = {round(error, 2)}')\n",
    "\n",
    "    iter_num += 1\n",
    "    w = new_w\n",
    "    \n",
    "w_list = np.array(w_list)\n",
    "\n",
    "print(f'Iter {iter_num}: error - {error}, weights: {new_w}')\n",
    "print(f'В случае использования стохастического градиентного спуска ошибка составляет {round(errors[-1], 4)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $L_2$-регуляризация (ridge, регуляризация Тихонова)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Метрики"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def r2(y_true, y_pred):\n",
    "    n = len(y_true)\n",
    "    r2 = 1 - ((1/n * np.sum((y_true - y_pred)**2))/(1/n * np.sum((y_true - np.mean(y_true))**2)))\n",
    "    return r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def accuracy_metric(actual, predicted):\n",
    "    correct = 0\n",
    "    for i in range(len(actual)):\n",
    "        if actual[i] == predicted[i]:\n",
    "            correct += 1\n",
    "    return correct / float(len(actual)) * 100.0"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Lesson_1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
